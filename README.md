This project showcases an image captioning model that leverages the attention mechanism to generate descriptive captions for images. Utilizing TensorFlow, the model adopts an encoder-decoder architecture where the encoder extracts features from the image using a pre-trained InceptionResNetV2 network. The attention mechanism, inspired by the Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention paper, allows the decoder to focus on different parts of the image when generating each word of the caption. This approach enhances the model's ability to produce accurate and contextually relevant descriptions. The training is performed on the COCO 2014 captions dataset, and the project includes components for training, evaluation, and generating captions in real-time.
